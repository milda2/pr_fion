from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# 1. Data Preprocessing
# Tokenize the sequences
tokenizer = Tokenizer(char_level=True)  # char_level=True means each character (amino acid) is treated as a token
tokenizer.fit_on_texts(train_terms)
X_train = tokenizer.texts_to_sequences(train_terms)

# Pad sequences for consistent length
X_train_padded = pad_sequences(X_train)

# 2. Model Design
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.layers import Bidirectional
#INPUT_SHAPE = [train_df.shape[1]]
BATCH_SIZE = 5120

vocab_size = len(tokenizer.word_index) + 1  # +1 for padding token
embedding_dim = 128  # You can adjust this
sequence_length = X_train_padded.shape[1]

model = tf.keras.Sequential([
    Embedding(vocab_size, embedding_dim, input_length=sequence_length),
    Bidirectional(LSTM(128, return_sequences=True)),  # Wrapping LSTM with Bidirectional
    Dropout(0.3),
    Bidirectional(LSTM(128)),  # Wrapping LSTM with Bidirectional
    Dropout(0.3),
    Dense(512, activation='relu'),
    Dropout(0.3),
    Dense(num_of_labels, activation='sigmoid')
])

# Continue with compilation, callbacks, and training as before...
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='binary_crossentropy',
    metrics=['binary_accuracy', tf.keras.metrics.AUC(name='auc')]
)

early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
model_checkpoint = ModelCheckpoint('best_model_rnn.h5', save_best_only=True, monitor='val_loss')

history = model.fit(
    X_train_padded, labels_df,
    batch_size=BATCH_SIZE,
    epochs=20,
    validation_split=0.2,
    callbacks=[early_stopping, model_checkpoint]
)
# checking 
import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.plot(history.history['binary_accuracy'])
plt.plot(history.history['val_binary_accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

# Plot training & validation loss values
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()
